In the previous lesson, I mentioned the
Kubernetes control plan, which is the fleet of
cooperating processes that make a Kubernetes
cluster work. Even though, you'll only work directly with a few
of these components, it helps to know about them
and the role each plays. I'll build up a Kubernetes
cluster part by part, explaining each piece as I go. After I'm done, I'll show you how a Kubernetes
cluster running in GKE is a lot less work to manage than one you
provisioned yourself. Okay, here we go. First and foremost,
your cluster needs computers. Nowadays, the computers
that compose your clusters are usually
virtual machines. They always are in GKE, but they could be
physical computers too. One computer is
called the master and the others are
called simply nodes. The job of the nodes
is to run pods, the job of the master is to
coordinate the entire cluster. We will meet it's control
plane components first. Several critical Kubernetes
components run on the master. The single component
that you've interact with directly is
the Kube-APIserver. This components job is
to accept commands that view or change the state
of the cluster, including launching pods. In this specialization, you will use the kubectl
command frequently. This commands job
is to connect to Kube-APIserver and
communicate with it using the Kubernetes API. Kube-APIserver also
authenticates incoming requests, determines whether they
are authorized and valid, and manages admission control. But it's not just kubectl that
talks with Kube-APIserver. In fact, any query or
change to the cluster state must be addressed to
the Kube-API server. ETCD is the cluster's database. It's job is to reliably store
the state of the cluster, this includes all of
the cluster configuration data and more dynamic information, such as what nodes are
part of the cluster, what pods should be running, and where they should be running. You never interact
directly with etcd. Instead, Kube-API
server interacts with the database on behalf
of the rest of the system. Kube-scheduler is responsible for scheduling pods onto the nodes. To do that, it evaluates the requirements of
each individual pod, and select which node
is most suitable. But it doesn't do
the work of actually launching pods onto nodes. Instead, whenever it discovers a pod object that doesn't yet have an assignment to a node, it chooses a node
and simply writes the name of that node
into the pot object. Another component of this system is responsible for then
launching the pods, and you will see it very soon. But how does Kube-scheduler
decide where to run a pod? It knows the state
of all the nodes, and it will also obey constraints that you
define on where a pod may run based on hardware,
software, and policy. For example, you might
specify that certain pod is only allowed to run on nodes with a certain amount of memory. You can also define a affinity specification
which cause groups of pods to prefer running on the same node or
anti affinity specifications, which ensure that pods do
not run on the same node. You'll learn more about some of these tools
in later modules. Kube-controller manager
has abroad or job. It continuously monitors
the state of a cluster through Kube-APIserver, who whenever the current
state of the cluster doesn't match the desired state, Kube-controller manager
will attempt to make changes to achieve
the desired state. It's called the controller
manager because many Kubernetes objects are maintained by loops of
code called controllers. These loops of code, handle the process
of remediation. Controllers will be
very useful to you. To be specific, you
all use certain kinds of Kubernetes controllers
to manage workloads. For example, remember
our problem of keeping three in
genetics pods always running, we can gather them together
into a controller object called a deployment that not
only keeps them running, but also lets us scale them and bring them together
underneath or front-end. We'll meet deployments
later in this module. Other kinds of controllers has system-level
responsibilities. For example,
node controller's job is to monitor and respond
when a node is offline. Kube-cloud-manager
manages controllers that interact with underlying
cloud providers. For example, if you
manually launched a Kubernetes cluster on
Google Compute Engine, Kube-cloud-manager
would be responsible for bringing in GCP features like load balancers and storage volumes
when you needed them. Each node runs a small family of control plane components too. For example, each node
runs a Kubelet. You can think of Kubelet as Kubernetes agent on each node. When the Kube-APIserver wants
to start a pod on a node, it connects to
that node's Kubelet. Kubelet uses the container
run-time to start the pod, and monitors its lifecycle, including readiness
and aliveness probes, and reports back
to Kube-APIserver. Do you remember our use
of the term container run time in the previous module? This is the software
that knows how to launch a container from
a container image. The world of Kubernetes offers several choices of
container runtimes, but the Linux distribution
that GKE uses for its nodes, launches containers
using container D, The run time component of docker. Kube-proxy's job is to maintain network connectivity among
the pods in a cluster. In open-source Kubernetes,
it does so using the firewall and
capabilities if IP tables, which are built into
the Linux kernel. Later in this specialization, we will learn how GKE
handles pod networking.